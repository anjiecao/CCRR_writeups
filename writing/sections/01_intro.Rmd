Cross-cultural differences are a striking part  of the broader landscape of human variation. Differences in values and behavior across cultures are obvious to even a casual observer, and researchers have attempted to quantify these differences via a wide range of measures. Comparisons between the United States and China – often as exemplars of Western and East Asian cultures – have been especially well-researched, with differences attested in a wide range of cognitive domains, including visual attention [@ji2000culture; @chua2005cultural; @waxman2016early], executive function [@tan2020chinese; @sabbagh2006development], language learning [@chan2011english;@tardif1996nouns; @chan2011english; @waxman2016early], relational reasoning [@carstensen2019context; @richland2010young; @cheng2020development; @su2020analogical], similarity judgments [@ji2004culture], values [@ji2001culture; @spencer2007culture; @kwan1997pancultural], preferences [@liang2012effect; @diyanni2015role; @corriveau2017cultural] and self-concepts [@spencer2009cultural; @spencer2009dialectical]. As a result, the US and China are increasingly becoming major cultural poles in efforts to assess and measure cultural differences [@muthukrishna2020beyond] and correct for the pervasive bias in psychology research toward US and European samples [@henrich2010weirdest; @arnett2016neglected; @nielsen2017persistent].

Despite a long empirical tradition of comparisons between these two cultures and an abundance of psychological accounts for observed differences, there is little consensus on how measurements relate to each other. Reports of cultural differences are often difficult to compare quantitatively because of the varying samples, measures, and methods used in different reports. Further, many of the most prominent reports of cross-cultural differences predate the field-wide discussion of methodological issues in psychology research during the past 10 years [@open2015estimating]. For example, much research in this tradition has been exploratory and hence has not followed current guidance regarding limiting analytic flexibility in order to decrease false positives [@simmons2011]. Given the importance of claims about specific cross-cultural differences for constructing theories of culture more broadly [e.g., @markus1992and;@markus2010cultures], replication of many empirical findings is likely warranted. 

There is already some empirical evidence suggesting issues in the robustness of cross-cultural measurements.  measures used in this literature are not standardized and do not have published evidence about reliability and validity [@flake2020measurement]. The few extant direct comparisons between measures of cultural difference suggest that theoretically related tasks, such as implicit and explicit measures of the same construct, often do not cohere [e.g., @kitayama2009cultural]. Further, in a study with twenty cross-cultural measures used within a single US sample, @na2010cultural found a lack of coherence between tasks measuring social orientation and cognitive style, observing only 8 significant correlations between tasks across 90 statistical tests ^[ The authors interpret their findings as imply the measures are orthogonal – indexing different constructs – and conclude that group-level differences between cultures are unlikely to relate to within-group individual differences. However an alternative possibility is that the reliabilities of many individual tasks are low, a feature which would ensure low correlations between them.
.]. Further, more recent work has raised concerns about low external validity of some cross-cultural measures and failed to replicate cultural differences on several related measures [@zhou2008perceiving; @mercier2012use; @mercier2015easterners]. Thus, there is a need for exploration of the reliability of individual tasks as well as the intercorrelations between them.

Our goal in the current study was to replicate a set of cross-cultural measures that had previously been used in comparisons of East Asian and Western cultures (most often comparisons between US and either Chinese or Japanese participants). We made the decision to pursue the strategy of gathering relatively large and heterogeneous convenience samples using online recruitment, rather than recruiting smaller, more matched samples using in-lab recruitment. Our reasoning was that the larger samples that we could access using online recruitment would allow us to conduct highly-powered statistical tests, allowing us to either reject or accept the null hypothesis of no cultural difference between measures. Further, larger samples would afford the analysis of individual and demographic differences within culture, a topic of considerable interest in this literature [e.g., @na2010cultural]. Finally, the development of browser-based online versions of prominent cross-cultural tasks would allow their inspection and reuse by other researchers, thus promoting a more cumulative approach to the measurement of cultural differences. 

The interpretation of any replication result is complex, given that disparate outcomes between an initial study and a replication can occur for many reasons – including but not limited to differences in experimental methods, sample or population differences, and simple sampling variation in the outcomes [@zwaan2018making,@nosek2020replication, @machery2020replication]. Our strategy of pursuing online convenience samples limits the interpretation of our replication results: nearly all of the tasks we selected had previously been administered in person, and the populations sampled in previous reports varied but were largely convenience samples of either college students or community members. More generally, our strategy of constructing a battery of replication studies and administering them uniformly means that specific decisions about sampling and administration will not be matched with the original studies (which were likely heterogeneous in their samples and administration). Thus, our replication studies should be taken as an assessment of whether a set of previously-reported cross-cultural differences can be recovered in convenience populations recruited online, rather than as assessments of the veracity of the original findings. Nevertheless, we believe that the field of cross-cultural psychology can be advanced via the identification of tasks that yield cross-cultural differences robustly across a variety of samples and administration formats and we hope our work contributes to this aim. We return to this interpretive issue in the General Discussion.

Our task selection process was initially shaped by an interest in relational reasoning and accounts explaining it with reference to cross-cultural differences in visual attention and social cognition [e.g., @kuwabara2012cross; @duffy2009development; @moriguchi2012cultural]. Additionally, in Experiment 1, we selected tasks that could potentially be administered to young children as well as adults, for use in future work addressing developmental questions about the relative time course of cross-cultural differences across the visual, social, and cognitive domains. We balanced four desiderata in our task selection, preferentially choosing tasks that (1) had been theoretically or empirically implicated in relational reasoning, (2) were associated with differential performance in US-China comparisons or related cultural contrasts (e.g., East Asian vs. Western cultures), (3) were relatively short, accessible tasks appropriate for web administration, and (4) were vision or social cognition accounts for relational reasoning. We further conducted a fairly extensive set of pilots to ensure that participants understood instructions and that the tasks yielded interpretable data. 

In Experiment 2, we selected a second set of tasks to investigate based in part on the results of Experiment 1. In particular, we repeated a handful of tasks from Experiment 1 to collect further evidence (in some cases, varying task parameters). We then selected a further set of tasks that probed both cross-cultural differences in higher-level cognition (e.g., language and reasoning) and perception, again respecting the desideratum that the tasks should be relatively short and amenable to administration in a web browser. The final set of tasks included in each Experiment is listed in Table 1.

In addition to the goal of replicating individual tasks in which cross-cultural differences in cognition and perception had been previously reported, our hope was that the relatively large dataset that we collected could be used to explore the structure of within- and across-cultural variation in cognition and perception more broadly. Towards this goal, we included a relatively extensive demographic questionnaire in both of our Experiments, with the aim of using these measures to explore variation within our samples. In the final section of the paper, we report a series of exploratory analyses. The first of these assess the reliability of individual tasks, aiming to gauge whether individual tasks are reliable enough from a psychometric point of view to support further individual differences analyses. We then report across-task correlations, aiming to discover covariation between tasks that might indicate that they load on the same construct. Finally, we turn to analyses of whether within-culture demographic variables predict variation in task performance. Overall, a number of tasks revealed acceptable levels of reliability, and tasks clustered together FIXMEXYZ FIXME. We found relatively limited evidence for demographic predictors of within-culture variation, however. 

We make all code and data from our experiments available for further data collection and analysis in hopes of promoting further cumulative work on measures and theories of cross-cultural variation. 



