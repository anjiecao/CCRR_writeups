

Cross-cultural differences are a striking part of the broader landscape of human variation. Differences in values and behavior across cultures are obvious to even a casual observer, and researchers have attempted to quantify these differences via a wide range of measures. Comparisons between Western and East Asian cultures  have been especially well-researched, with differences attested in a wide range of cognitive domains, including visual attention [@ji2000culture; @chua2005cultural; @waxman2016early], executive function [@tan2020chinese; @sabbagh2006development], language learning [@chan2011english;@tardif1996nouns; @chan2011english; @waxman2016early], relational reasoning [@carstensen2019context; @richland2010young; @cheng2020development; @su2020analogical], similarity judgments [@ji2004culture], values [@ji2001culture; @spencer2007culture; @kwan1997pancultural], preferences [@liang2012effect; @diyanni2015role; @corriveau2017cultural] and self-concepts [@spencer2009cultural; @spencer2009dialectical]. As a result, Western and East Asian cultures are increasingly treated as cultural poles in efforts to measure cultural differences [@muthukrishna2020beyond] and to correct for the pervasive bias in psychology research toward US and European samples [@henrich2010weirdest; @arnett2016neglected; @nielsen2017persistent].

Despite a long empirical tradition of comparisons between these cultures and an abundance of psychological accounts for observed differences, estimates of differences are difficult to compare quantitatively because of the varying samples, measures, and methods used in different reports. Further, many of the most prominent reports of cross-cultural differences predate the field-wide discussion of methodological issues in psychology research during the past 10 years [@open2015estimating]. For example, much research in this tradition has been exploratory and hence has not followed current guidance regarding limiting analytic flexibility in order to decrease false positives [@simmons2011]. Given the importance of evidence about specific cross-cultural differences for constructing theories of culture more broadly [e.g., @markus1992and;@markus2010cultures], further investigation of many empirical findings is likely warranted.

Some empirical evidence points to issues in the robustness of cross-cultural measurements.  Typically, measures used in this literature are not standardized and do not have published evidence about reliability and validity [@flake2020measurement]. The few extant direct comparisons between measures of cultural difference suggest that theoretically related tasks, such as implicit and explicit measures of the same construct, might not cohere [e.g., @kitayama2009cultural]. Further, in a study with twenty cross-cultural measures used within a single US sample, @na2010cultural found a lack of coherence between tasks measuring social orientation and cognitive style, observing only 8 significant correlations between tasks across 90 statistical tests.^[These authors interpreted their findings as implying that the measures are orthogonal -- indexing different constructs -- and concluded that group-level differences between cultures are unlikely to relate to within-group individual differences. However, an alternative possibility is that the reliabilities of many individual tasks are low, a feature which would ensure low correlations between them.] Finally, more recent work failed to replicate cultural differences on several related measures [@zhou2008perceiving; @mercier2012use; @mercier2015easterners]. Thus, there is a need to explore the reliability of individual tasks as well as the intercorrelations between them.

Our goal in the current study was to collect a large dataset on a range of cross-cultural measures that had previously been used in comparisons of East Asian and Western cultures, enabling investigations of the robustness of these differences in new samples of Chinese and US participants. We decided to gather relatively large and heterogeneous convenience samples using online recruitment, rather than recruiting smaller, more matched samples using in-lab recruitment. Our reasoning was that the larger samples that we could access using online recruitment would allow us to conduct highly-powered statistical tests, allowing us to make well-powered tests for cultural differences. Further, larger samples afford the analysis of individual and demographic differences within culture, a topic of considerable interest in this literature (e.g., Na et al., 2010, 2020). Finally, the development of browser-based online versions of prominent cross-cultural tasks would allow their inspection and reuse by other researchers, thus promoting a more cumulative approach to the measurement of cultural differences. 

Our experiments were intended to be close replications of the original studies, but differences in format of administration introduced inevitable variation, in some cases more substantial than others. The interpretation of discrepant outcomes between an original study and a replication is complex, given that disparate outcomes can occur for many reasons [@zwaan2018making; @nosek2020replication; @machery2020replication]. In our case, interpretation is especially difficult and we explicitly avoid interpreting our results as bearing on the status of the original findings we investigate.

There were some significant differences between our experiments and the original studies. First, we recruited online convenience samples from the U.S. and China. Previous work varied in the participants' country of origin (in several cases, Japan for East Asian participants; Canada for Western participants), largely recruited either college students or community members, and was administered more than a decade ago. Within-culture variation and generational differences between our samples and previous samples make results difficult to compare directly. Furthermore, our strategy of constructing a battery of replication studies and administering them uniformly online altered the contexts in which participants engaged with the tasks and in some cases required alterations to the tasks themselves. 

Accordingly, our replication studies should be viewed as an assessment of robustness: specifically, we assess whether a set of previously-reported East-West cross-cultural differences can be recovered in online convenience populations. These are not  assessments of the veracity of the original findings. Nevertheless, we believe that cross-cultural psychology can be advanced via the identification of tasks that yield cross-cultural differences robustly across a variety of samples and administration formats -- we hope our work contributes to this aim. We return to these interpretive issues in the General Discussion.


Our task selection process was initially shaped by an interest in relational reasoning and accounts explaining it with reference to cross-cultural differences in visual attention and social cognition [@kuwabara2012cross; @duffy2009development; @moriguchi2012cultural]. Additionally, in Experiment 1, we selected tasks that could potentially be administered to young children as well as adults, for use in future work addressing developmental questions about the relative time course of cross-cultural differences across the visual, social, and cognitive domains. We balanced three desiderata in our task selection, preferentially choosing tasks that (1) had been theoretically or empirically implicated in relational reasoning, (2) were associated with differential performance in US-China comparisons or related cultural contrasts (i.e., East Asian vs. Western cultures), and (3) were relatively short, accessible tasks appropriate for web administration. We also conducted an extensive set of pilot tests to ensure that participants understood instructions and that the tasks yielded interpretable data. 

In Experiment 2, we selected a second set of tasks to investigate based in part on the results of Experiment 1. In particular, we repeated a handful of tasks from Experiment 1, in some cases, varying task parameters. We then selected a further set of tasks that probed both cross-cultural differences in higher-level cognition (e.g., language and reasoning) and perception, again respecting the desideratum that the tasks should be relatively short and amenable to administration in a web browser. The final set of tasks included in each experiment is listed in Table 1.

In addition to the goal of replicating individual tasks, our hope was that the relatively large dataset we collected could be used to explore the structure of within- and between-culture variation in cognition and perception more broadly. Towards this goal, we included a relatively extensive demographic questionnaire in both of our experiments, with the aim of using these measures to explore variation within our samples. In the final section of the paper, we report a series of exploratory analyses. The first of these assesses the reliability of individual tasks to gauge whether these tasks are reliable enough from a psychometric point of view to support further individual differences analyses. We then report correlations across tasks, aiming to discover covariation between tasks that might indicate that they load on the same construct. Finally, we turn to analyses of whether within-culture demographic variables predict variation in task performance. Overall, a number of tasks revealed acceptable levels of reliability, but tasks did not cluster together, and we found relatively few demographic predictors of within-culture variation.


We make all code and data from our experiments available for further data collection and analysis in hopes of promoting further cumulative work on measures and theories of cross-cultural variation. 


\begin{longtable}{l p{1.2in} p{1.4in} p{1.4in} p{.2in} p{.2in}}
    \caption{Tasks included in each experiment and the final sample size after exclusions.}\\
    \small  % Switch from 12pt to 11pt; otherwise, table won't fit
    \setlength\LTleft{0pt}            
    \setlength\LTright{0pt}         
    
    \bf{Experiment} & \bf{Task} & \bf{Citation} & \bf{Task Description} & \bf{CN} & \bf{US} \\
    \hline
        1 & Ambiguous Relational Match-To-Sample (cRMTS) & Carstensen et al. (2019) & Infer whether an object or relation is causally relevant &  167  &  169 \\

& Picture Free Description & Imada, Carlson, \& Itakura (2013) & Describe pictures from memory after a brief study period &  167 &  169\\

& Ebbinghaus Illusion & Imada, Carlson, \& Itakura (2013) & Judge the size of circles in a context designed to bias size judgments &  167  &  169\\

& Horizon Collage & Senzaki, Masuda, \& Nand (2014) & Make an image by dragging and dropping stickers onto a display &  167  &  169\\

& Symbolic Self-Inflation (Family) & Kitayama et al. (2009) & Draw self and family members as circles &  141 &  110\\

& Uniqueness Preference & Kim \& Markus (1999) & Choose a sticker from five stickers, four of which are the same color &  167 &  169\\

& Child Causal Attribution & Seiver, Gopnik, \& Goodman (2013) & Watch short vignettes and explain the decisions of the characters &  167 &  169\\

& Raven's Progressive Matrices & Su (2020) & Use analogical reasoning to complete visually-presented patterns &  167 &  169\\
2 & Ambiguous Relational Match-To-Sample (cRMTS) & Carstensen et al. (2019) & Infer whether an object or relation is causally relevant &  174 &  293\\

& Picture Free Description & Imada, Carlson, \& Itakura (2013) & Describe pictures from memory after a brief study period &  132 &  284\\

& Change Detection & Mausda \& Nisbett (2007) & Find differences in the foreground or background of two images &  160 &  253\\

& Symbolic Self-Inflation (Friends) & Kitayama et al. (2009) & Draw a sociogram with self and friends as nodes, relationships as edges &  158 &  252\\

& Adult Causal Attribution & Morris \& Peng (1994) & Read a crime story and explain the criminalâ€™s motivations &  114 &  293\\

& Taxonomic-Thematic Similarity & Ji, Zhang, \& Nisbett (2004) & Match items based on taxonomic or thematic similarity (e.g., cow: chicken / grass) & 178 &  295\\

& Semantic Intuition & Li, Liu, Chalmers, \& Snedeker (2018) & Decide whether a story refers to a named character (whose actions are mischaracterized) or the person who performed the actions (but had a different name) &  181 &  298\\

& Raven's Progressive Matrices & Su (2020) & Use analogical reasoning to complete visually-presented patterns &  181 &  298\\
    \hline
    \end{longtable}