FIXME: The majority of results are visualized in Figure 1, except for the Ebbinghaus Illusion data, in Figure 2. Below we discuss the results of each task in turn. 

```{r}
e1_lmer_df <- readRDS(here("cached_results/exp1/glmer_model_summary.RDS"))
e1_bfs_df <- readRDS(here("cached_results/exp1/bfs_summary.RDS"))

d <- read_csv(here("data/03_processed_data/exp1/tidy_main.csv"))
```


### Ambiguous cRMTS


```{r rmts_summary, warning=FALSE}
RMTS_summary <- d %>% 
  filter(task_name == "RMTS") %>% 
  group_by(culture, subject) %>% 
  summarise(
    relational_match = mean(resp)
  ) %>% 
  group_by(culture) %>% 
  summarise(
    culture_mean = round(mean(relational_match),2), 
    culture_sd = round(sd(relational_match),2)
  )

rmts_tidy_model <- e1_lmer_df %>% filter(model_name == "rmts_model")
rmts_bf <- e1_bfs_df %>% filter(bfs_type == "rmts")
```


To examine whether adults in the US and China show differing preferences for object-based or relational solutions, we ran a mixed-effects logistic regression predicting response choice (object or relation) with country (US or China) as a fixed effect. There was no main effect of country on response choice (object or relation; US: *M* = `r filter(RMTS_summary, culture == "US")$culture_mean`, *SD* = `r filter(RMTS_summary, culture == "US")$culture_sd`; CN: *M* = `r filter(RMTS_summary, culture == "CN")$culture_mean`, *SD* = `r filter(RMTS_summary, culture == "CN")$culture_sd`; $\beta$ = `r filter(rmts_tidy_model, predictor == "cultureUS")$estimate_print`, *SE* = `r filter(rmts_tidy_model, predictor == "cultureUS")$se_print`, *z* = `r filter(rmts_tidy_model, predictor == "cultureUS")$z_print`, *p* `r filter(rmts_tidy_model, predictor == "cultureUS")$p_print`). The Bayes Factor analysis suggested that the evidence was in favor of the null hypothesis (*BF* = `r rmts_bf$bf_print`). The preference for object-based solutions seen in US preschoolers and the corresponding preference for relational solutions observed in China in an ambiguous context did not extend to adults in our samples.

Our US results replicate findings by @goddu2018toddlers, who reported that US adults are at chance in this paradigm. It seems likely that adults in both groups of our study are aware of the ambiguous evidence and their near-chance selections reflect (reasonable) uncertainty. 

### Picture Free Description 

```{r fd_summary, warning = FALSE}
fd_summary <- d %>% 
  filter(task_name == "FD") %>% 
  group_by(culture, subject, resp_type) %>% 
  summarise(
    mean_resp = mean(resp)
  ) %>% 
  group_by(culture, resp_type) %>% 
  summarise(
    culture_mean = round(mean(mean_resp),2), 
    culture_sd = round(sd(mean_resp), 2)
  )

tidy_fd_mention_model <- e1_lmer_df %>% filter(model_name == "fd_mention_model")
tidy_fd_descriptive_model <- e1_lmer_df %>% filter(model_name == "fd_descriptive_model")
fd_mention_bf <- e1_bfs_df %>% filter(bfs_type == "fd_firstmention")
fd_descriptive_bf <- e1_bfs_df %>% filter(bfs_type == "fd_descriptives")
```

 Based on Imada et al [-@imada2013east], we expected Chinese participants would be more likely to mention background objects first and provide more descriptive accounts for background objects relative to focal objects, in comparison with US participants. Our results extend previous findings with the former metric (first mention; Proportion of Relational Match choice: US: *M* = `r filter(fd_summary, culture == "US"&resp_type == "first_mention_focal")$culture_mean`, *SD* = `r filter(fd_summary, culture == "US"&resp_type == "first_mention_focal")$culture_sd`; CN: *M* = `r filter(fd_summary, culture == "CN"&resp_type == "first_mention_focal")$culture_mean`, *SD* = `r filter(fd_summary, culture == "CN"&resp_type == "first_mention_focal")$culture_sd`) but not the latter (number of descriptive accounts; For focal objects: US: *M* = `r filter(fd_summary, culture == "US"&resp_type == "imada_focal_description")$culture_mean`, *SD* = `r filter(fd_summary, culture == "US"&resp_type == "imada_focal_description")$culture_sd`, ; CN: *M* = `r filter(fd_summary, culture == "CN"&resp_type == "imada_focal_description")$culture_mean`, *SD* = `r filter(fd_summary, culture == "CN"&resp_type == "imada_focal_description")$culture_sd`; For background objects: US: *M* = `r filter(fd_summary, culture == "US"&resp_type == "imada_bckgrd_description")$culture_mean`, *SD* = `r filter(fd_summary, culture == "US"&resp_type == "imada_bckgrd_description")$culture_sd`; CN: *M* = `r filter(fd_summary, culture == "CN"&resp_type == "imada_bckgrd_description")$culture_mean`, *SD* = `r filter(fd_summary, culture == "CN"&resp_type == "imada_bckgrd_description")$culture_sd`). 

For first mention, we ran a mixed-effects logistic regression predicting the type of first mention (object or relation) with country (US or China) as a fixed effect. We found a main effect of country ($\beta$ = `r filter(tidy_fd_mention_model, predictor == "cultureUS")$estimate_print`, *SE* = `r filter(tidy_fd_mention_model, predictor == "cultureUS")$se_print`, *z* = `r filter(tidy_fd_mention_model, predictor == "cultureUS")$z_print`, *p* `r filter(tidy_fd_mention_model, predictor == "cultureUS")$p_print`). For descriptive accounts, we ran a mixed-effect Poisson regression model predicting the number of descriptive accounts, with description type (focal or background), country (US or China), and their interaction as fixed effects. There was a significant main effect of culture (with US participants providing more descriptions overall: $\beta$ = `r filter(tidy_fd_descriptive_model, predictor == "cultureUS")$estimate_print`, *SE* = `r filter(tidy_fd_descriptive_model, predictor == "cultureUS")$se_print`, *t* = `r filter(tidy_fd_descriptive_model, predictor == "cultureUS")$z_print`, *p* `r filter(tidy_fd_descriptive_model, predictor == "cultureUS")$p_print`). The culture effect interacted with the description types, but the effect was in the opposite direction, with U.S participants provided more background descriptions than focal descriptions, relative to Chinese participants ($\beta$ = `r filter(tidy_fd_descriptive_model, predictor == "description_typeimada_focal_description:cultureUS")$estimate_print`, *SE* = `r filter(tidy_fd_descriptive_model, predictor == "description_typeimada_focal_description:cultureUS")$se_print`, *t* = `r filter(tidy_fd_descriptive_model, predictor == "description_typeimada_focal_description:cultureUS")$z_print`, *p* `r filter(tidy_fd_descriptive_model, predictor == "description_typeimada_focal_description:cultureUS")$p_print`). The Bayes Factor analysis was consistent with the frequentist models (For first mention: *BF* = `r fd_mention_bf$bf_print`; For description type: *BF* = `r fd_descriptive_bf$bf_print`). 

The mixed results between the first mention and descriptive accounts measures suggest that there is some complexity in linking broader theoretical accounts to specific measures; we interpret this result with caution and include the task in Experiment 2 to follow up further.

### Ebbinghaus Illusion 

```{r ebbinghaus_summary, warning=FALSE}

ebb_summary <- d %>% 
  filter(task_name == "EBB") %>% 
  group_by(culture, subject, task_info) %>% 
  summarise(
    mean_resp = mean(resp)
  ) %>% 
  group_by(culture, task_info) %>% 
  summarise(
    culture_mean = round(mean(mean_resp),2)
  )

ebb_tidy_model <- e1_lmer_df %>% filter(model_name == "ebb_model")
ebb_bf <- e1_bfs_df %>% filter(bfs_type == "ebb")
```

To test whether perception of the Ebbinghaus illusion varied across populations in our sample, we ran a mixed-effects logistic regression predicting accuracy on each trial, with country (US or China), context (No Context or Illusion Context), and circle size difference (the percent of difference in diameters) as fixed effects, along with their interactions. We found main effects of context (with worse performance in the Illusion Context; $\beta$ = `r filter(ebb_tidy_model, predictor == "contextNC")$estimate_print`, *SE* = `r filter(ebb_tidy_model, predictor == "contextNC")$se_print`, *z* = `r filter(ebb_tidy_model, predictor == "contextNC")$z_print`, *p* `r filter(ebb_tidy_model, predictor == "contextNC")$p_print`) and circle size difference (worse performance for smaller differences; $\beta$ = `r filter(ebb_tidy_model, predictor == "size_diff")$estimate_print`, *SE* = `r filter(ebb_tidy_model, predictor == "size_diff")$se_print`, *z* = `r filter(ebb_tidy_model, predictor == "size_diff")$z_print`, *p* `r filter(ebb_tidy_model, predictor == "size_diff")$p_print`). There was a marginally significant main effect of country at the opposite of the predicted direction (US participants performed worse: $\beta$ = `r filter(ebb_tidy_model, predictor == "cultureUS")$estimate_print`, *SE* = `r filter(ebb_tidy_model, predictor == "cultureUS")$se_print`, *z* = `r filter(ebb_tidy_model, predictor == "cultureUS")$z_print`, *p* `r filter(ebb_tidy_model, predictor == "cultureUS")$p_print`) but no interactions with country (All $\beta$ < 0.01; All *p* > 0.05). The Bayes Factor suggested that the results were extremely in favor of the null hypothesis (*BF* = `r ebb_bf$bf_print`). 

In sum, we failed to replicate  cultural differences found between Western (US/UK) and Japanese participants in susceptibility to the Ebbinghaus illusion.
<!-- in our sample of US and Chinese adults. -->

### Horizon Collage 

```{r}
hz_summary <- d %>% 
  filter(task_name == "HZ") %>% 
  group_by(culture, subject, task_info, resp_type) %>% 
  summarise(
    mean_resp = mean(resp)
  ) %>% 
  group_by(culture, task_info, resp_type) %>% 
  summarise(
    culture_mean = round(mean(mean_resp),2), 
    culture_sd = round(sd(mean_resp), 2)
  ) %>% 
  mutate(
    culture_mean = case_when(
      (resp_type == "stkr_area") ~ round((culture_mean/(75*75*3*3)),2), #3 from scaling, 75 from conversion: http://auctionrepair.com/pixels.html, 
      TRUE ~ culture_mean 
    ), 
    culture_sd = case_when(
      (resp_type == "stkr_area") ~ round(sd(culture_mean),2), #3 from scaling, 75 from conversion: http://auctionrepair.com/pixels.html, 
      TRUE ~ culture_sd 
    )
    )

hz_height_bf <- e1_bfs_df %>% filter(bfs_type == "hz_height")
hz_n_bf <- e1_bfs_df %>% filter(bfs_type == "hz_n")
hz_stkr_area_bf <- e1_bfs_df %>% filter(bfs_type == "hz_area")
```


In the Horizon Collage task, three key measurements are calculated from the “collage” participants created: the height of the horizon (height in proportion to the height of the frame), the number of stickers, and the total area of the stickers covered (following the original analysis, we added up the area occupied by each individual sticker); Japanese children tend to put the horizon higher and include more stickers that cover more area in their collage, compared with Canadian children. We ran a fixed effect linear model with culture as the main predictor for each of the measurements. Culture did not significantly predict any of the three measurements (Sticker height: US: *M* = `r filter(hz_summary, culture == "US"&resp_type == "hz_height")$culture_mean`, *SD* = `r filter(hz_summary, culture == "US"&resp_type == "hz_height")$culture_sd`; CN: *M* = `r filter(hz_summary, culture == "CN"&resp_type == "hz_height")$culture_mean`, *SD* = `r filter(hz_summary, culture == "CN"&resp_type == "hz_height")$culture_sd`; Sticker number: US: *M* = `r filter(hz_summary, culture == "US"&resp_type == "stkr_count")$culture_mean`, *SD* = `r filter(hz_summary, culture == "US"&resp_type == "stkr_count")$culture_sd`; CN: *M* = `r filter(hz_summary, culture == "CN"&resp_type == "stkr_count")$culture_mean`, *SD* = `r filter(hz_summary, culture == "CN"&resp_type == "stkr_count")$culture_sd`; Sticker area: US: *M* = `r filter(hz_summary, culture == "US"&resp_type == "stkr_area")$culture_mean`, *SD* = `r filter(hz_summary, culture == "US"&resp_type == "stkr_area")$culture_sd`; CN: *M* = `r filter(hz_summary, culture == "CN"&resp_type == "stkr_area")$culture_mean`, *SD* = `r filter(hz_summary, culture == "CN"&resp_type == "stkr_area")$culture_sd`; All $\beta$ < 0.03; All *p* > 0.1). All bayes factors suggest that there are only anecdotal evidence supporting the culture effect (Sticker height: *BF* = `r hz_height_bf$bf_print`; Sticker number: *BF* = `r hz_n_bf$bf_print`; Sticker area: *BF* = `r hz_stkr_area_bf$bf_print`).

Our experiment contrasted Chinese and US adults, rather than Japanese and Canadian children. Although @senzaki2014holistic found that the cultural differences were more salient in older children than younger children, suggesting that cultural differences might increase with development, interpretation of our failure to replicate is still qualified by differences in culture and medium of administration. 

### Symbolic Self-Inflation 

```{r ssi_summary, warning=FALSE}
ssi_summary <- d %>% 
  filter(task_name == "SI") %>% 
  group_by(culture, subject, task_info, resp_type) %>% 
  summarise(
    mean_resp = mean(resp)
  ) %>% 
  group_by(culture, task_info, resp_type) %>% 
  summarise(
    culture_mean = round(mean(mean_resp),2), 
    culture_sd = round(sd(mean_resp),2)
  )

ssi_ratio_tidy_model <- e1_lmer_df %>% filter(model_name == "ssi_ratio_model") 
ssi_ratio_bf <-  e1_bfs_df %>% filter(bfs_type == "ssi_ratio")
```

To test whether US adults have a larger symbolic self than Chinese adults, we ran a linear regression predicting percent inflation score (calculated by dividing the diameter of the self circle by the average diameter of circles for others) with country (US or China) as a fixed effect. No difference was found in the degree of symbolic self-inflation between US and China adults based on percent inflation scores (US: *M* = `r filter(ssi_summary, culture == "US", resp_type == "inflation_score_ratio")$culture_mean`, *SD* = `r filter(ssi_summary, culture == "US", resp_type == "inflation_score_ratio")$culture_sd`; CN: *M* = `r filter(ssi_summary, culture == "CN", resp_type == "inflation_score_ratio")$culture_mean`, *SD* = `r filter(ssi_summary, culture == "CN", resp_type == "inflation_score_ratio")$culture_sd`; $\beta$  = `r filter(ssi_ratio_tidy_model, predictor == "cultureUS")$estimate_print`, *SE* = `r filter(ssi_ratio_tidy_model, predictor == "cultureUS")$se_print`, *t* = `r filter(ssi_ratio_tidy_model, predictor == "cultureUS")$z_print`, *p* `r filter(ssi_ratio_tidy_model, predictor == "cultureUS")$p_print`). The Bayes Factor shows moderate evidence in favor of the null hypothesis (*BF* = `r ssi_ratio_bf$bf_print`)

One possible explanation for our null results is that we adopted a different task design from @kitayama2009cultural. Instead of asking participants to draw their social network, our design asked participants to draw themselves and the family members they grew up with. During the coding process, we noticed that people from both cultures tended to draw older people, e.g., their parents, into larger circles, which might have resulted in overall larger circles for other people than the self-circles in our task for both cultures, masking any US-China difference in the degree of self-inflation. It is possible that there are also cultural differences between Japan and China in self concept; Japanese samples typically demonstrate characteristics previously associated with East Asian cultures in general, with Chinese samples deviating from these characteristics at times [@bailey1997conceptions; @church2012self; @church2014relating]. 

### Uniqueness Preference 

```{r up_summary, warning=FALSE}

up_summary <- d %>% 
  filter(task_name == "CP") %>% 
  group_by(culture) %>% 
  summarise(
    culture_mean = round(mean(resp),2), 
    culture_sd = round(sd(resp),2))

up_tidy_model <- e1_lmer_df %>% filter(model_name == "up_model") 
up_bf <- e1_bfs_df %>% filter(bfs_type == "up")
```


We examined cross-cultural preferences for uniqueness by running a simple logistic regression predicting each participant’s single choice (minority or majority color) with country (US or China) as a fixed effect; we used logistic regression rather than mixed effects logistic regression due to the absence of repeated observations. There was not a large cross-cultural difference in the probability of choosing the uniquely colored sticker (US: *M* = `r filter(up_summary, culture == "US")$culture_mean`, *SD* = `r filter(up_summary, culture == "US")$culture_sd`; CN: *M* = `r filter(up_summary, culture == "CN")$culture_mean`, *SD* = `r filter(up_summary, culture == "CN")$culture_sd`; $\beta$ = `r filter(up_tidy_model, predictor == "cultureUS")$estimate_print`, *SE* = `r filter(up_tidy_model, predictor == "cultureUS")$se_print`, *z* = `r filter(up_tidy_model, predictor == "cultureUS")$z_print`, *p* `r filter(up_tidy_model, predictor == "cultureUS")$p_print`). Our Bayes Factor analysis suggested that we have no evidence supporting the test hypothesis that culture is a meaningful predictor in participants' choice (*BF* = `r up_bf$bf_print`). 

The difference between our result and that of the original study by @kim1999deviance might be related to the use of online format in our study. In the original study, participants were asked to pick a gift pen from five physical pens with different barrel colors. It could be that Asian American participants in the previous study chose the more common color because they wanted the next person to also have room for decision making in the face of resource scarcity, or because they were expressing values or identities influenced by East Asian cultural mandates favoring interpersonal harmony and similarity. Our finding is also consistent with previous work demonstrating that tendencies toward conformity in East Asian samples are linked to reputation management [@yamagishi2008preferences]; it may be that our online experiment did not establish a sufficient social context to motivate participant concern about reputation, and accordingly failed to motivate reputation management in the form of a conformity preference. 

### Causal Attribution 

```{r ca_summary, warning=FALSE}
CA_summary <- d %>% 
  group_by(culture, task_info, resp_type, subject) %>% 
  filter(task_info == "CA") %>%
  summarise(
    sbj_resp = mean(resp, na.rm = TRUE)
  ) %>% 
  ungroup() %>% 
  group_by(culture, resp_type) %>% 
  summarise(
    culture_mean = round(mean(sbj_resp),2), 
    culture_sd = round(sd(sbj_resp),2)
  )

ca_tidy_model <- e1_lmer_df %>% filter(model_name == "ca_model") 
ca_bf <-  e1_bfs_df %>% filter(bfs_type == "ca")
```

To test whether Chinese participants tended to make more situational attributions, and US adults more personal attributions, we ran a mixed-effects Poisson regression predicting the number of attributions included in each explanation, with attribution type (situation or person), country (US or CN), and their interaction as fixed effects. We found a main effect of attribution type (Situation attribution: US: *M* = `r filter(CA_summary, culture == "US" & resp_type == "situation_attribution")$culture_mean`, *SD* = `r filter(CA_summary, culture == "US" & resp_type == "situation_attribution")$culture_sd`; CN: *M* = `r filter(CA_summary, culture == "CN" & resp_type == "situation_attribution")$culture_mean`, *SD* = `r filter(CA_summary, culture == "CN" & resp_type == "situation_attribution")$culture_sd`; Person attribution: US: *M* = `r filter(CA_summary, culture == "US" & resp_type == "person_attribution")$culture_mean`, *SD* = `r filter(CA_summary, culture == "US" & resp_type == "person_attribution")$culture_sd`; CN: *M* = `r filter(CA_summary, culture == "CN" & resp_type == "person_attribution")$culture_mean`, *SD* = `r filter(CA_summary, culture == "CN" & resp_type == "person_attribution")$culture_sd`; $\beta$ = `r filter(ca_tidy_model, predictor == "attrib_typesituation_attribution")$estimate_print`, *SE* = `r filter(ca_tidy_model, predictor == "attrib_typesituation_attribution")$se_print`, *z* = `r filter(ca_tidy_model, predictor == "attrib_typesituation_attribution")$z_print`, *p* `r filter(ca_tidy_model, predictor == "attrib_typesituation_attribution")$p_print`). Neither the interaction nor the main effect of culture was significant (Both $\beta$ < 0.3; *p* > 0.05). The Bayes Factor analysis shows strong support for the null hypothesis (*BF* = `r ca_bf$bf_print`)

The failure to find cross-cultural differences in attributions could be related to the style of the tasks, which was relatively repetitive and originally designed for children; in Experiment 2, we follow up with a causal attribution task designed for adults.

### Raven's Standard Progressive Matrices

```{r rv_summary, warning=FALSE}
rv_summary <- d %>% 
  filter(task_name == "RV") %>% 
  group_by(culture, subject) %>% 
  summarise(
    accuracy = mean(resp)
  ) %>% 
  group_by(culture) %>% 
  summarise(
    culture_mean = round(mean(accuracy),2), 
    culture_sd = round(sd(accuracy), 2)
  )

rv_tidy_model <- e1_lmer_df %>% filter(model_name == "rv_model") 
rv_bf <-  e1_bfs_df %>% filter(bfs_type == "rv")
```

As an exploratory measure of relational reasoning, we ran a mixed-effects logistic regression predicting per-trial accuracy, with country as a fixed effect, random intercepts for each subject and question, and by-question random slopes for country. We found a main effect of country, with Chinese participants outperforming those from the US (US: *M* = `r filter(rv_summary, culture == "US")$culture_mean`, *SD* = `r filter(rv_summary, culture == "US")$culture_sd`; CN: *M* = `r filter(rv_summary, culture == "CN")$culture_mean`, *SD* = `r filter(rv_summary, culture == "CN")$culture_sd`; $\beta$ = `r filter(rv_tidy_model, predictor == "cultureUS")$estimate_print`, *SE* = `r filter(rv_tidy_model, predictor == "cultureUS")$se_print`, *z* = `r filter(rv_tidy_model, predictor == "cultureUS")$z_print`, *p* `r filter(rv_tidy_model, predictor == "cultureUS")$p_print`). The bayes factor is consistent with the frequentist model, showing strong support for the test hypothesis (*BF* = `r rv_bf$bf_print`). 

Our findings replicate @su2020analogical in finding an advantage for Chinese participants on Raven’s Matrices. In our context, we also interpret the relatively high scores we observed as evidence that participants were engaging fully with our tasks. 

